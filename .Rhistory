write_graph(coocNet, path, format = "graphml")
#%#%#%#%#%#%#%#%#%#%
# Loading packages
#%#%#%#%#%#%#%#%#%#%
library(tidyverse)    # numerous data wrangling packages
library(data.table)   # quick data loading
library(igraph)       # Network package
library(network)      # Network package
library(ggnetwork)    # Plotting networks
library(RColorBrewer) # Nice colours for plots
library(ggpubr)       # Arrange multiple plots
# Useful commands
'%!in%' <- function(x,y)!('%in%'(x,y)) # opposite of %in% command
options(stringsAsFactors = FALSE)
#%#%#%#%#%#%#%#%#%#%
# Load data directly from github
#%#%#%#%#%#%#%#%#%#%
node_info <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/node_info.csv")
edge_list <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/edge_list.csv")
#%#%#%#%#%#%#%#%#%#%
# Changing year of appearance form long to wide for easier visualization
#%#%#%#%#%#%#%#%#%#%
node_info$value <- 1
node_info$year <- node_info$year_appeared
node_info <- node_info %>% arrange(year) %>% pivot_wider(names_from = year, values_from = value) %>% mutate_at(vars(starts_with("20")), ~replace(., is.na(.), 0))
#%#%#%#%#%#%#%#%#%#%
# Changing lift threshold for connection
#%#%#%#%#%#%#%#%#%#%
# We could try a minimum lift of 10 or something derived from the distribution to get a more sparse network and clearer structure
#edge_list <- edge_list %>% filter(lift > 10)
#%#%#%#%#%#%#%#%#%#%
# Create Network
#%#%#%#%#%#%#%#%#%#%
# Creating dedicated dataframes for nodes and edges including node attributes and optional edge weights
nodes <- node_info
edges <- edge_list %>% select(tag1,tag2,co_occurance_count,lift) #%>% rename(weight=lift) # the renaming makes a huge difference because suddenly R uses the lift as edge weights in the network construction
# Creating the network
coocNet <- graph_from_data_frame(d=edges, vertices=nodes, directed=FALSE)
class(coocNet)
V(coocNet)$names
V(coocNet)$labels
coocNet$names
class(coocNet)
vertex_attr(coocNet)
V(coocNet)$labels <- V(coocNet)$names
V(coocNet)$labels
V(coocNet)$names
names(coocNet)
lables(coocNet)
labels(coocNet)
#%#%#%#%#%#%#%#%#%#%
# Load data directly from github
#%#%#%#%#%#%#%#%#%#%
node_info <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/node_info.csv")
edge_list <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/edge_list.csv")
#%#%#%#%#%#%#%#%#%#%
# Changing year of appearance form long to wide for easier visualization
#%#%#%#%#%#%#%#%#%#%
node_info$value <- 1
node_info$year <- node_info$year_appeared
node_info <- node_info %>% arrange(year) %>% pivot_wider(names_from = year, values_from = value) %>% mutate_at(vars(starts_with("20")), ~replace(., is.na(.), 0))
#%#%#%#%#%#%#%#%#%#%
# Changing lift threshold for connection
#%#%#%#%#%#%#%#%#%#%
# We could try a minimum lift of 10 or something derived from the distribution to get a more sparse network and clearer structure
edge_list <- edge_list %>% filter(lift > 20)
#%#%#%#%#%#%#%#%#%#%
# Create Network
#%#%#%#%#%#%#%#%#%#%
# Creating dedicated dataframes for nodes and edges including node attributes and optional edge weights
nodes <- node_info
edges <- edge_list %>% select(tag1,tag2,co_occurance_count,lift) #%>% rename(weight=lift) # the renaming makes a huge difference because suddenly R uses the lift as edge weights in the network construction
# Creating the network
coocNet <- graph_from_data_frame(d=edges, vertices=nodes, directed=FALSE)
class(coocNet)
#%#%#%#%#%#%#%#%#%#%
# Loading packages
#%#%#%#%#%#%#%#%#%#%
library(tidyverse)    # numerous data wrangling packages
library(data.table)   # quick data loading
library(igraph)       # Network package
library(network)      # Network package
library(ggnetwork)    # Plotting networks
library(RColorBrewer) # Nice colours for plots
library(ggpubr)       # Arrange multiple plots
# Useful commands
'%!in%' <- function(x,y)!('%in%'(x,y)) # opposite of %in% command
options(stringsAsFactors = FALSE)
#%#%#%#%#%#%#%#%#%#%
# Load data directly from github
#%#%#%#%#%#%#%#%#%#%
node_info <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/node_info.csv")
edge_list <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/edge_list.csv")
#%#%#%#%#%#%#%#%#%#%
# Changing year of appearance form long to wide for easier visualization
#%#%#%#%#%#%#%#%#%#%
node_info$value <- 1
node_info$year <- node_info$year_appeared
node_info <- node_info %>% arrange(year) %>% pivot_wider(names_from = year, values_from = value) %>% mutate_at(vars(starts_with("20")), ~replace(., is.na(.), 0))
#%#%#%#%#%#%#%#%#%#%
# Changing lift threshold for connection
#%#%#%#%#%#%#%#%#%#%
# We could try a minimum lift of 10 or something derived from the distribution to get a more sparse network and clearer structure
edge_list <- edge_list %>% filter(lift > 20)
#%#%#%#%#%#%#%#%#%#%
# Create Network
#%#%#%#%#%#%#%#%#%#%
# Creating dedicated dataframes for nodes and edges including node attributes and optional edge weights
nodes <- node_info
edges <- edge_list %>% select(tag1,tag2,co_occurance_count,lift) #%>% rename(weight=lift) # the renaming makes a huge difference because suddenly R uses the lift as edge weights in the network construction
# Creating the network
coocNet <- graph_from_data_frame(d=edges, vertices=nodes, directed=FALSE)
class(coocNet)
#%#%#%#%#%#%#%#%#%#%
# Running community detection with&without lift as weight parameter
clusterlouvain <- cluster_louvain(coocNet) # without weight
clusterlouvain_2 <- cluster_louvain(coocNet, weights=edge_list$lift) # use lift as weight - many more communities
# Assign community membership as an attribute
V(coocNet)$louvain_community <- membership(clusterlouvain)
V(coocNet)$louvain_community_lift <- membership(clusterlouvain_2)
# Calculating and assigning betweenness centrality - THIS CAN TAKE A WHILE - comment out if not needed
b_centrality <- betweenness(coocNet)
V(coocNet)$b_centrality <- b_centrality
#%#%#%#%#%#%#%#%#%#%
# Export for use in Gephi
#%#%#%#%#%#%#%#%#%#%
# Set your path via setting your working directory (setwd()) or choose any other path where to save the network
path = paste(getwd(),"/cooc_network_without_weight_Lift20.graphml", sep="")
write_graph(coocNet, path, format = "graphml")
# Loading packages
#%#%#%#%#%#%#%#%#%#%
library(tidyverse)    # numerous data wrangling packages
library(data.table)   # quick data loading
library(igraph)       # Network package
library(network)      # Network package
library(ggnetwork)    # Plotting networks
library(RColorBrewer) # Nice colours for plots
library(ggpubr)       # Arrange multiple plots
# Useful commands
'%!in%' <- function(x,y)!('%in%'(x,y)) # opposite of %in% command
options(stringsAsFactors = FALSE)
#%#%#%#%#%#%#%#%#%#%
# Load data directly from github
#%#%#%#%#%#%#%#%#%#%
node_info <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/node_info.csv")
edge_list <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/edge_list.csv")
View(edge_list)
library(tidyverse)    # numerous data wrangling packages
library(data.table)   # quick data loading
library(igraph)       # Network package
library(network)      # Network package
library(ggnetwork)    # Plotting networks
library(RColorBrewer) # Nice colours for plots
library(ggpubr)       # Arrange multiple plots
# Useful commands
'%!in%' <- function(x,y)!('%in%'(x,y)) # opposite of %in% command
options(stringsAsFactors = FALSE)
#%#%#%#%#%#%#%#%#%#%
# Load data directly from github
#%#%#%#%#%#%#%#%#%#%
node_info <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/node_info.csv")
edge_list <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/edge_list.csv")
%#%#%#%#%#%#%#%#%#%
# Changing year of appearance form long to wide for easier visualization
#%#%#%#%#%#%#%#%#%#%
node_info$value <- 1
node_info$year <- node_info$year_appeared
node_info <- node_info %>% arrange(year) %>% pivot_wider(names_from = year, values_from = value) %>% mutate_at(vars(starts_with("20")), ~replace(., is.na(.), 0))
#%#%#%#%#%#%#%#%#%#%
# Changing lift threshold for connection
#%#%#%#%#%#%#%#%#%#%
# We could try a minimum lift of 10 or something derived from the distribution to get a more sparse network and clearer structure
#edge_list <- edge_list %>% filter(lift > 10)
#%#%#%#%#%#%#%#%#%#%
# Create Network
#%#%#%#%#%#%#%#%#%#%
# Creating dedicated dataframes for nodes and edges including node attributes and optional edge weights
nodes <- node_info
edges <- edge_list %>% select(tag1,tag2,co_occurance_count,lift) #%>% rename(weight=lift) # the renaming makes a huge difference because suddenly R uses the lift as edge weights in the network construction
# Creating the network
coocNet <- graph_from_data_frame(d=edges, vertices=nodes, directed=FALSE)
class(coocNet)
# transform to minimal spanning tree
coocNet <- mst(coocNet)
# Run community detection (Louvain) and betweenness centrality
#%#%#%#%#%#%#%#%#%#%
# Running community detection with&without lift as weight parameter
clusterlouvain <- cluster_louvain(coocNet) # without weight
clusterlouvain_2 <- cluster_louvain(coocNet, weights=edge_list$lift) # use lift as weight - many more communities
# Assign community membership as an attribute
V(coocNet)$louvain_community <- membership(clusterlouvain)
V(coocNet)$louvain_community_lift <- membership(clusterlouvain_2)
# Calculating and assigning betweenness centrality - THIS CAN TAKE A WHILE - comment out if not needed
b_centrality <- betweenness(coocNet)
V(coocNet)$b_centrality <- b_centrality
#%#%#%#%#%#%#%#%#%#%
# Export for use in Gephi
#%#%#%#%#%#%#%#%#%#%
# Set your path via setting your working directory (setwd()) or choose any other path where to save the network
path = paste(getwd(),"/cooc_network_MST.graphml", sep="")
write_graph(coocNet, path, format = "graphml")
library(tidyverse)    # numerous data wrangling packages
library(data.table)   # quick data loading
library(lubridate)    # Working with dates
library(arules)       # Association rules (for network links)
library(widyr)        # Super fast functions for long to wide operations
# Useful commands
'%!in%' <- function(x,y)!('%in%'(x,y)) # opposite of %in% command
options(stringsAsFactors = FALSE)
# Read the huge data frame using the data.table package
df <- fread("/Users/oleteutloff/Desktop/Scraping_BERUFENET/results_berufenet.json")
View(df)
# Read the huge data frame using the data.table package
df <- fromJSON(fread("/Users/oleteutloff/Desktop/Scraping_BERUFENET/results_berufenet.json"))
# Read the huge data frame using the data.table package
df <- read.json("/Users/oleteutloff/Desktop/Scraping_BERUFENET/results_berufenet.json")
# Read the huge data frame using the data.table package
df <- read_json("/Users/oleteutloff/Desktop/Scraping_BERUFENET/results_berufenet.json")
# Read the huge data frame using the data.table package
df <- read.json("/Users/oleteutloff/Desktop/Scraping_BERUFENET/results_berufenet.json")
edge_list <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/edge_list.csv")
#%#%#%#%#%#%#%#%#%#%
# Loading packages
#%#%#%#%#%#%#%#%#%#%
library(tidyverse)    # numerous data wrangling packages
library(data.table)   # quick data loading
library(igraph)       # Network package
library(network)      # Network package
library(ggnetwork)    # Plotting networks
library(RColorBrewer) # Nice colours for plots
library(ggpubr)       # Arrange multiple plots
# Useful commands
'%!in%' <- function(x,y)!('%in%'(x,y)) # opposite of %in% command
options(stringsAsFactors = FALSE)
edge_list <- fread("https://raw.githubusercontent.com/QuantomOle/The-Technology-Space-and-Digital-Development/main/data/edge_list.csv")
# Loading packages
#%#%#%#%#%#%#%#%#%#%
library(tidyverse)    # numerous data wrangling packages
library(data.table)   # quick data loading
library(lubridate)    # Working with dates
library(arules)       # Association rules (for network links)
library(widyr)        # Super fast functions for long to wide operations
# Useful commands
'%!in%' <- function(x,y)!('%in%'(x,y)) # opposite of %in% command
options(stringsAsFactors = FALSE)
#%#%#%#%#%#%#%#%#%#%
# Load data and simple data cleaning
#%#%#%#%#%#%#%#%#%#%
# Read the huge data frame using the data.table package
df <- fread("/Users/oleteutloff/Desktop/Data/Stack_overflow/2018-08-13 Contributions.csv")
# Filter those rows that actually have tags
df <- df %>% filter(tags != "")
# keep only relevant columns
df <- df %>% dplyr::select(id, creation_date, Country, tags)
# extract year from date
df$year <- format(as.Date(df$creation_date),"%Y")
# Limit to 2017 (for now) - Not necessary anymore
#df <- df %>% filter(year == 2017)
# exclude duplicate ids: there 132 ids that are not unique. I exclude them
df <- df %>% distinct()
# Still duplicates remaining
ids_duplicate <- as.data.frame(table(df$id)) %>% filter(Freq > 1) %>% select(Var1)
df <- df %>% filter(id %!in% ids_duplicate$Var1)
####################################
# transform tags from string into list of individual tags
df$tags_list <- list(strsplit(df$tags,"\\|"))
# Break the list into one large vector
tags <- unlist(df$tags_list)
# Add the number of tech-tags per post to the post dataframe
df$number_tags <- sapply(df$tags_list,length)
# Turn df into a data frame (it used to be a 'data.table')
df <- data.frame(df)
# Use the broken-up tag list and the number of categories per firm to create one large panel dataframe
df.expanded <- df[rep(row.names(df), df$number_tags),]
df.expanded$tags <- tags
### Exclude all tags that do not appear in our wikipedia matching table
# final solution when wiki-matching is complete
#df_wiki <- fread("/Users/oleteutloff/Desktop/Data/wikipedia_matching/wiki_match.csv")
#df_wiki <- df_wiki %>% filter(Wikipedia != "")
# df.expanded <- df.expanded %>% filter(tags %in% df_wiki$tag)
# provisional solution - Some how we lose 47 technology tags
df_wiki <- fread("/Users/oleteutloff/Desktop/Data/wikipedia_matching/wiki_provisional.csv")
df.expanded <- df.expanded %>% filter(tags %in% df_wiki$tag)
############################################
### Create tag matrix to calculate lift ###
############################################
# select only necessary columns
df_matrix <- df.expanded %>% dplyr::select(tags,id)
# delete everything else to free up memory - not strictly necessary
rm(df,df_wiki,df.expanded, ids_duplicate, tags)
## Counting co-occurance using very promising approach from widyr library - It works super fast! Avoids creating super large sparse matrix
cooccurance_count <- widyr::pairwise_count(df_matrix, tags, id) # function from library(widyr)
# Counting total occurance of each tag
single_count <- df_matrix %>% count(tags)
## Merging the two together (twice to have count of item1 and item2)
single_count <- rename(single_count, tag1_count = n)
# merge
cooccurance_count <- cooccurance_count %>% left_join(single_count, by = c("item1" = "tags"))
# rename for clarity
single_count <- rename(single_count, tag2_count = tag1_count)
# merge again to have also count for item 2
cooccurance_count <- cooccurance_count %>% left_join(single_count, by = c("item2" = "tags"))
# Adding total number of observations
cooccurance_count$total_num_obs <- length(unique(df_matrix$id))
# Renaming remaining columns
cooccurance_count <- rename(cooccurance_count, tag1 = item1, tag2 = item2, co_occurance_count = n)
# Transform integers to doubles - to avoid integer overflow error that introduces NAs
cooccurance_count$tag1_count <- as.double(cooccurance_count$tag1_count)
cooccurance_count$tag2_count <- as.double(cooccurance_count$tag2_count)
cooccurance_count$total_num_obs <- as.double(cooccurance_count$total_num_obs)
View(cooccurance_count)
View(cooccurance_count)
# calculate proximity
cooccurance_count$proximity <- cooccurance_count$co_occurance_count / cooccurance_count$tag1_count
cooccurance_count$proximity2 <- cooccurance_count$co_occurance_count / cooccurance_count$tag2_count
View(cooccurance_count)
max(cooccurance_count$proximity)
min(cooccurance_count$proximity)
max(cooccurance_count$proximity2)
min(cooccurance_count$proximity2)
mean(cooccurance_count$proximity)
mean(cooccurance_count$proximity2)
# Density curve
ggplot(cooccurance_count, aes(x=proximity)) + geom_density()
length(cooccurance_count$proximity > 0.5)
length(cooccurance_count$proximity > 0.8)
length(cooccurance_count$proximity > 1)
count(cooccurance_count$proximity > 0.5)
table(cooccurance_count$proximity)
table(cooccurance_count$proximity > 0.5)
table(cooccurance_count$proximity > 0.55)
t(apply(cooccurance_count, 1, sort)
cooccurance_count[t(apply(cooccurance_count, 1, sort),]
cooccurance_count[t(apply(cooccurance_count, 1, sort)),]
test <- cooccurance_count[t(apply(cooccurance_count, 1, sort)),]
View(test)
View(cooccurance_count)
ind <- duplicated(cooccurance_count[,1:2])
ind <- !duplicated(cooccurance_count[,1:2])
rm(test)
rm(ind)
View(cooccurance_count)
test1 <- cooccurance_count %>% filter(tag1="visual-studio-2010", tag2="wpf")
test1 <- cooccurance_count %>% filter(tag1=="visual-studio-2010", tag2=="wpf")
View(test1)
test2 <- cooccurance_count %>% filter(tag2=="visual-studio-2010", tag1=="wpf")
View(test2)
View(test1)
test <- transform(cooccurance_count, proximity = pmin(proximity1, proximity2))
# calculate proximity
cooccurance_count$proximity1 <- cooccurance_count$co_occurance_count / cooccurance_count$tag1_count
test <- transform(cooccurance_count, proximity = pmin(proximity1, proximity2))
View(test)
test1 <- cooccurance_count %>% filter(tag1=="visual-studio-2010", tag2=="wpf")
test1 <- test %>% filter(tag1=="visual-studio-2010", tag2=="wpf")
test2 <- test %>% filter(tag2=="visual-studio-2010", tag1=="wpf")
max(test$proximity)
min(test$proximity)
table(test$proximity > 0.5)
table(test$proximity > 0.3)
View(single_count)
8890*8890
View(test1)
View(test2)
# transform into undirected by removing duplicates and always keeping the minimum proximity
cooccurance_count <- transform(cooccurance_count, proximity = pmin(proximity1, proximity2))
View(cooccurance_count)
cooccurance_count <- select(cooccurance_count, -c(proximity1,proximity2))
View(cooccurance_count)
cooccurance_count %>% filter(proximity > 0.5)
# Calculating lift
cooccurance_count$lift <- (cooccurance_count$co_occurance_count*cooccurance_count$total_num_obs)/(cooccurance_count$tag1_count*cooccurance_count$tag2_count)
# removing the duplicates in edge list (because we will build an undirected network)
no_duplicates <- cooccurance_count[!duplicated(t(apply(cooccurance_count, 1, sort))),]
data_check <- c(no_duplicates$tag1, no_duplicates$tag2)
length(unique(data_check)) == length(unique(df_matrix$tags))
View(no_duplicates)
# Export data to .csv for upload to Github and use to build the technology space network
write.csv(no_duplicates,"/Users/oleteutloff/Desktop/Data/edge_list_full.csv", row.names = FALSE)
edge_list <- fread("/Users/oleteutloff/Desktop/Data/edge_list_full.csv")
edge_list <- fread("/Users/oleteutloff/Desktop/Data/edge_list_full.csv")
library(tidyverse)    # numerous data wrangling packages
library(data.table)   # quick data loading
library(igraph)       # Network package
library(network)      # Network package
library(ggnetwork)    # Plotting networks
library(RColorBrewer) # Nice colours for plots
library(ggpubr)       # Arrange multiple plots
# Useful commands
'%!in%' <- function(x,y)!('%in%'(x,y)) # opposite of %in% command
options(stringsAsFactors = FALSE)
edge_list <- fread("/Users/oleteutloff/Desktop/Data/edge_list_full.csv")
plot(ecdf(edge_list[,"proximity"]))
View(edge_list)
plot(ecdf(edge_list$proximity))
plot(ecdf(edge_list$lift))
hist(edge_list$proximity)
p <- ggplot(edge_list, aes(x=proximity)) +
geom_density()
# Add mean line
p+ geom_vline(aes(xintercept=mean(proximity)),
color="blue", linetype="dashed", size=1)
p+ geom_vline(aes(xintercept=mean(proximity)),
color="blue", linetype="dashed", size=1)+
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))
)
ggplot(edge_list, aes(x=proximity)) +
geom_density() +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))
)
ggplot(edge_list, aes(x=proximity)) +
geom_density() +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))
) +
geom_vline(aes(xintercept=mean(proximity)),
color="blue", linetype="dashed", size=1)
ggplot(edge_list, aes(x=proximity)) +
geom_density() +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))
)
ggplot(edge_list, aes(x=lift)) +
geom_density()
ggplot(edge_list, aes(x=lift)) +
geom_density() +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))
)
10^0
install.packages('pheatmap')
library(pheatmap)
df <- scale(mtcars)
View(df)
ggplot(data = edge_list, aes(x=tag1, y=tag2, fill=proximity)) +
geom_raster()
ggplot(data = edge_list, aes(x=tag1, y=tag2, fill=proximity)) +
geom_raster()
ggplot(data = edge_list, aes(x=tag1, y=tag2, fill=proximity)) +
geom_raster()
ggplot(data = edge_list, aes(x=tag1, y=tag2, fill=proximity)) + geom_raster()
# lift
ggplot(data = edge_list, aes(x=tag1, y=tag2, fill=lift)) + geom_raster()
ggplot(edge_list %>%
arrange(proximity) %>%
mutate(rn = row_number())) +
geom_step(aes(x=proximity, y=rn))
ggplot(edge_list %>%
desc(proximity) %>%
mutate(rn = row_number())) +
geom_step(aes(x=proximity, y=rn))
ggplot(edge_list %>%
arrange(desc(proximity)) %>%
mutate(rn = row_number())) +
geom_step(aes(x=proximity, y=rn))
ggplot(edge_list %>%
arrange(desc(proximity)) %>%
mutate(rn = row_number())) +
geom_step(aes(x=proximity, y=rn))  +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))) +
scale_y_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x)))
# proximity
ggplot(edge_list %>%
arrange(desc(proximity)) %>%
mutate(rn = row_number())) +
geom_step(aes(x=proximity, y=rn))  +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))) +
scale_y_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))) +
labs(x = "proximity threshold", y = "links")
# lift
ggplot(edge_list %>%
arrange(desc(lift)) %>%
mutate(rn = row_number())) +
geom_step(aes(x=lift, y=rn))  +
scale_x_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))) +
scale_y_log10(
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))) +
labs(x = "lift threshold", y = "links")
unique(edge_list$tag1)
sum(unique(edge_list$tag1))
count(unique(edge_list$tag1))
length(unique(edge_list$tag1))
ggplot(data = edge_list, aes(x=tag1, y=tag2, fill=proximity)) + geom_raster()+
scale_fill_viridis(discrete=FALSE) +
theme_ipsum()
install.packages("hrbrthemes")
install.packages("hrbrthemes")
## Without clustering
library(hrbrthemes)
install.packages("ggplot2")
## Without clustering
library(hrbrthemes)
## Without clustering
library(hrbrthemes)
